{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🧪 Taller #2 - Corre tu primer modelo de lenguaje local sin GPU (con llama.cpp)\n",
        "\n",
        "En este taller exprés vamos a demostrar que no necesitas una supercomputadora para experimentar con inteligencia artificial.\n",
        "\n",
        "Usando `llama.cpp` y Google Colab, correremos modelos de lenguaje pequeños (como TinyLlama o Gemma) directamente desde la CPU, sin GPU y con menos de 12 GB de RAM. Todo en menos de 15 minutos.\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 ¿Qué vas a aprender?\n",
        "- Qué es `llama.cpp` y por qué es clave para correr modelos localmente.\n",
        "- Cómo descargar y preparar un modelo `.gguf`.\n",
        "- Cómo generar texto con un LLM pequeño desde la terminal.\n",
        "- Tips para probar, optimizar y liberar memoria en entornos limitados.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ Requisitos\n",
        "- Tener una cuenta de Google.\n",
        "- Saber copiar y pegar código (sí, eso es todo).\n",
        "- Ganas de explorar cómo funciona un modelo de lenguaje por dentro.\n",
        "\n",
        "---\n",
        "\n",
        "### 🌎 ¿Por qué hacemos esto?\n",
        "En Zenomy Labs queremos mostrar que la IA no es solo para gigantes.  \n",
        "En LATAM también podemos construir, probar y aprender con nuestras propias herramientas.\n",
        "\n",
        "Este taller es una invitación a hacerlo juntos.\n",
        "\n",
        "---\n",
        "\n",
        "👉 ¡Corre la siguiente celda para comenzar!\n"
      ],
      "metadata": {
        "id": "0qlQF-_Tke6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 ¿Qué es `llama.cpp`?\n",
        "\n",
        "`llama.cpp` es una implementación ligera y optimizada en C/C++ para correr modelos de lenguaje grandes (LLMs) directamente desde tu CPU.  \n",
        "Fue creado para permitir que cualquiera pueda usar modelos como LLaMA sin depender de GPUs costosas.\n",
        "\n",
        "¿Por qué es útil para este taller?\n",
        "\n",
        "- Es compatible con modelos en formato `.gguf`, que están optimizados para uso local.\n",
        "- Funciona bien con modelos pequeños (1B - 3B de parámetros) incluso en entornos con poca RAM.\n",
        "- Permite correr modelos en laptops, Raspberry Pi o... Google Colab sin GPU 😎\n"
      ],
      "metadata": {
        "id": "TXE9-MLelIch"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oV5w4gocip1t"
      },
      "outputs": [],
      "source": [
        "# Instalar llama-cpp-python desde PyPI con soporte CPU\n",
        "!pip install llama-cpp-python --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📦 Paso 2: Descargar el modelo `Qwen3-1.7B` (versión open source)\n",
        "\n",
        "Vamos a usar el modelo [`Qwen3-1.7B`](https://huggingface.co/unsloth/Qwen3-1.7B-GGUF) publicado por el equipo de Unsloth.  \n",
        "Este modelo es open source y está entrenado para ser eficiente y útil incluso en contextos con recursos limitados.\n",
        "\n",
        "Utilizaremos la versión quantizada `Q4_0`, que reduce el uso de memoria manteniendo una calidad razonable.\n",
        "\n",
        "**Nota:** El archivo pesa aproximadamente 1.6 GB. Asegúrate de tener espacio suficiente en el entorno de ejecución.\n"
      ],
      "metadata": {
        "id": "X3ME7byYlZb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear carpeta para modelos si no existe\n",
        "!mkdir -p models\n",
        "\n",
        "# Descargar el modelo quantizado Q4_K_M desde Hugging Face\n",
        "!curl -L -o models/Qwen3-1.7B-GGUF.gguf https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_0.gguf?download=true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5_etKklle8E",
        "outputId": "539105c3-86ce-4172-849c-55cbc78940c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1331  100  1331    0     0  10640      0 --:--:-- --:--:-- --:--:-- 10733\n",
            "100 1007M  100 1007M    0     0  71.8M      0  0:00:14  0:00:14 --:--:--  109M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🤖 Paso 3: Generar texto con `Qwen3-1.7B`\n",
        "\n",
        "Ahora que tenemos el modelo y `llama.cpp` en su paquete de python, vamos a generar texto con un prompt simple.\n",
        "\n",
        "Este modelo está optimizado para ser pequeño pero útil. No esperes una superinteligencia, pero sí una buena respuesta en pocos segundos.\n",
        "\n",
        "Vamos a hacerle una pregunta sencilla:\n",
        "> *\"¿Qué es la inteligencia artificial?\"*\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "M_MF-sMvmFpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Cargar el modelo desde el archivo GGUF\n",
        "llm = Llama(\n",
        "    model_path=\"/content/models/Qwen3-1.7B-GGUF.gguf\",\n",
        "    n_ctx=512,\n",
        "    n_threads=4,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Generar texto con un prompt simple\n",
        "output = llm(\"¿Qué es la inteligencia artificial?\", max_tokens=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FDJHvxDmJFp",
        "outputId": "d581ef9b-b1b8-4cce-ab20-4f0cfc30ec3f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output['choices'][0]['text'].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhBzb6rkrWh1",
        "outputId": "95c89952-1524-4629-cc3b-8742d6670667"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Por qué es importante para el futuro?\n",
            "\n",
            "La inteligencia artificial (IA) es una disciplina de la ciencia de la computación y la inteligencia artificial que se enfoca en el desarrollo de máquinas y sistemas que pueden realizar tareas que requieren inteligencia humana, como reconocer patrones, tomar decisiones y aprender de experiencia. La IA se basa en la creación de algoritmos y modelos computacionales que permiten que estas máquinas procesen información\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔁 Prueba con otros modelos\n",
        "\n",
        "Ahora que ya sabes cómo cargar y usar un modelo `.gguf` con `llama-cpp-python`, puedes **repetir el proceso con cualquier otro modelo compatible**.\n",
        "\n",
        "### ¿Qué puedes probar?\n",
        "\n",
        "- Descargar otro modelo `.gguf` desde [Hugging Face](https://huggingface.co/models?library=ggml&sort=downloads).\n",
        "- Cargarlo con el mismo código que usamos para `Qwen3-1.7B`.\n",
        "- Cambiar el prompt, tokens o tamaño de contexto.\n",
        "- Medir diferencias en:\n",
        "  - ⏱️ Tiempo de respuesta\n",
        "  - 📏 Calidad de la salida\n",
        "  - 🧠 Tamaño del modelo y uso de RAM\n"
      ],
      "metadata": {
        "id": "Uli3XeEDtu-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora vamos a descargar un modelo con 8b de parametros para probar la capacidad de google colab y el tiempo de respuesta\n",
        "!curl -L -o models/Meta-Llama-3-8B-Instruct.Q5_0.gguf  https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_0.gguf?download=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmD6jVjgt1K4",
        "outputId": "80aad13c-0c0e-4625-e682-fa89ad346a92"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1363  100  1363    0     0   9893      0 --:--:-- --:--:-- --:--:--  9948\n",
            "100 5339M  100 5339M    0     0  47.0M      0  0:01:53  0:01:53 --:--:-- 39.9M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=\"/content/models/Meta-Llama-3-8B-Instruct.Q5_0.gguf\",\n",
        "    n_ctx=512,\n",
        "    n_threads=4,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "output = llm(\"¿Qué es la inteligencia artificial?\", max_tokens=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fe20UKCtuzc",
        "outputId": "fece198c-ff79-4168-d938-9dd10b693e4e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output['choices'][0]['text'].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fct4-GpMsyyV",
        "outputId": "0adb190b-7a05-4a4d-b24d-fd4f4a88bec8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| ¿Qué es la inteligencia artificial? | ¿Qué es la inteligencia artificial? | ¿Qué es la inteligencia artificial? | ¿Qué es la inteligencia artificial?\n",
            "¿Qué es la inteligencia artificial?\n",
            "La inteligencia artificial (IA) es el campo de la ciencia y la tecnología que se enfoca en crear máquinas que puedan realizar tareas que habitualmente requieren la inteligencia humana, como el aprendizaje, la toma de decisiones\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFmi9M23wQ4p",
        "outputId": "d8bb7952-4738-4343-dcf9-f48c3471d4a6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-91c097a6-15f7-4028-b9ac-73451a90bd00',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1752258741,\n",
              " 'model': '/content/models/Meta-Llama-3-8B-Instruct.Q5_0.gguf',\n",
              " 'choices': [{'text': ' | ¿Qué es la inteligencia artificial? | ¿Qué es la inteligencia artificial? | ¿Qué es la inteligencia artificial? | ¿Qué es la inteligencia artificial?\\n¿Qué es la inteligencia artificial?\\nLa inteligencia artificial (IA) es el campo de la ciencia y la tecnología que se enfoca en crear máquinas que puedan realizar tareas que habitualmente requieren la inteligencia humana, como el aprendizaje, la toma de decisiones',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'length'}],\n",
              " 'usage': {'prompt_tokens': 9, 'completion_tokens': 100, 'total_tokens': 109}}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## ✅ Conclusiones\n",
        "\n",
        "Acabas de correr un modelo de lenguaje local desde Google Colab, **sin GPU** y usando solo tu CPU y unos pocos GB de RAM.  \n",
        "Usaste `llama-cpp-python` para cargar un modelo `.gguf`, generar texto y entender cómo funcionan los LLMs a nivel básico.\n",
        "\n",
        "Esto puede parecer simple, pero no lo es.  \n",
        "Demuestra que **todos podemos experimentar con IA, incluso con recursos limitados**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 ¿Y ahora qué?\n",
        "\n",
        "Si te quedó gustando, podés:\n",
        "\n",
        "- Probar otros modelos `.gguf` desde [TheBloke en HuggingFace](https://huggingface.co/TheBloke)\n",
        "- Ajustar los parámetros: `n_ctx`, `n_threads`, `max_tokens`\n",
        "- Hacer una app simple sobre esto: un bot, un script, un experimento\n",
        "- Compartir este notebook y mejorarlo con tu comunidad\n",
        "\n",
        "---\n",
        "\n",
        "## 🙌 Zenomy Labs\n",
        "\n",
        "Este taller es parte de un esfuerzo más grande:  \n",
        "**hacer crecer una comunidad de IA en LATAM que cree, experimente y comparta en abierto.**\n",
        "\n",
        "Si te gustó esto, síguenos en redes y súmate a construir esta movida.  \n",
        "Nos faltan manos, ideas y energía.\n",
        "\n",
        "👉 https://zenomyai.com\n",
        "\n",
        "---\n",
        "\n",
        "Gracias por llegar hasta acá 🙌\n"
      ],
      "metadata": {
        "id": "cwgpnnXcszSn"
      }
    }
  ]
}