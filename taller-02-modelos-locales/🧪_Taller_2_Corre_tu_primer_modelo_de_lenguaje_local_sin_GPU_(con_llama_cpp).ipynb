{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§ª Taller #2 - Corre tu primer modelo de lenguaje local sin GPU (con llama.cpp)\n",
        "\n",
        "En este taller exprÃ©s vamos a demostrar que no necesitas una supercomputadora para experimentar con inteligencia artificial.\n",
        "\n",
        "Usando `llama.cpp` y Google Colab, correremos modelos de lenguaje pequeÃ±os (como TinyLlama o Gemma) directamente desde la CPU, sin GPU y con menos de 12 GB de RAM. Todo en menos de 15 minutos.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŽ¯ Â¿QuÃ© vas a aprender?\n",
        "- QuÃ© es `llama.cpp` y por quÃ© es clave para correr modelos localmente.\n",
        "- CÃ³mo descargar y preparar un modelo `.gguf`.\n",
        "- CÃ³mo generar texto con un LLM pequeÃ±o desde la terminal.\n",
        "- Tips para probar, optimizar y liberar memoria en entornos limitados.\n",
        "\n",
        "---\n",
        "\n",
        "### âš™ï¸ Requisitos\n",
        "- Tener una cuenta de Google.\n",
        "- Saber copiar y pegar cÃ³digo (sÃ­, eso es todo).\n",
        "- Ganas de explorar cÃ³mo funciona un modelo de lenguaje por dentro.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŒŽ Â¿Por quÃ© hacemos esto?\n",
        "En Zenomy Labs queremos mostrar que la IA no es solo para gigantes.  \n",
        "En LATAM tambiÃ©n podemos construir, probar y aprender con nuestras propias herramientas.\n",
        "\n",
        "Este taller es una invitaciÃ³n a hacerlo juntos.\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ‘‰ Â¡Corre la siguiente celda para comenzar!\n"
      ],
      "metadata": {
        "id": "0qlQF-_Tke6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§  Â¿QuÃ© es `llama.cpp`?\n",
        "\n",
        "`llama.cpp` es una implementaciÃ³n ligera y optimizada en C/C++ para correr modelos de lenguaje grandes (LLMs) directamente desde tu CPU.  \n",
        "Fue creado para permitir que cualquiera pueda usar modelos como LLaMA sin depender de GPUs costosas.\n",
        "\n",
        "Â¿Por quÃ© es Ãºtil para este taller?\n",
        "\n",
        "- Es compatible con modelos en formato `.gguf`, que estÃ¡n optimizados para uso local.\n",
        "- Funciona bien con modelos pequeÃ±os (1B - 3B de parÃ¡metros) incluso en entornos con poca RAM.\n",
        "- Permite correr modelos en laptops, Raspberry Pi o... Google Colab sin GPU ðŸ˜Ž\n"
      ],
      "metadata": {
        "id": "TXE9-MLelIch"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oV5w4gocip1t"
      },
      "outputs": [],
      "source": [
        "# Instalar llama-cpp-python desde PyPI con soporte CPU\n",
        "!pip install llama-cpp-python --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“¦ Paso 2: Descargar el modelo `Qwen3-1.7B` (versiÃ³n open source)\n",
        "\n",
        "Vamos a usar el modelo [`Qwen3-1.7B`](https://huggingface.co/unsloth/Qwen3-1.7B-GGUF) publicado por el equipo de Unsloth.  \n",
        "Este modelo es open source y estÃ¡ entrenado para ser eficiente y Ãºtil incluso en contextos con recursos limitados.\n",
        "\n",
        "Utilizaremos la versiÃ³n quantizada `Q4_0`, que reduce el uso de memoria manteniendo una calidad razonable.\n",
        "\n",
        "**Nota:** El archivo pesa aproximadamente 1.6 GB. AsegÃºrate de tener espacio suficiente en el entorno de ejecuciÃ³n.\n"
      ],
      "metadata": {
        "id": "X3ME7byYlZb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear carpeta para modelos si no existe\n",
        "!mkdir -p models\n",
        "\n",
        "# Descargar el modelo quantizado Q4_K_M desde Hugging Face\n",
        "!curl -L -o models/Qwen3-1.7B-GGUF.gguf https://huggingface.co/unsloth/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q4_0.gguf?download=true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5_etKklle8E",
        "outputId": "539105c3-86ce-4172-849c-55cbc78940c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1331  100  1331    0     0  10640      0 --:--:-- --:--:-- --:--:-- 10733\n",
            "100 1007M  100 1007M    0     0  71.8M      0  0:00:14  0:00:14 --:--:--  109M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ¤– Paso 3: Generar texto con `Qwen3-1.7B`\n",
        "\n",
        "Ahora que tenemos el modelo y `llama.cpp` en su paquete de python, vamos a generar texto con un prompt simple.\n",
        "\n",
        "Este modelo estÃ¡ optimizado para ser pequeÃ±o pero Ãºtil. No esperes una superinteligencia, pero sÃ­ una buena respuesta en pocos segundos.\n",
        "\n",
        "Vamos a hacerle una pregunta sencilla:\n",
        "> *\"Â¿QuÃ© es la inteligencia artificial?\"*\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "M_MF-sMvmFpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Cargar el modelo desde el archivo GGUF\n",
        "llm = Llama(\n",
        "    model_path=\"/content/models/Qwen3-1.7B-GGUF.gguf\",\n",
        "    n_ctx=512,\n",
        "    n_threads=4,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Generar texto con un prompt simple\n",
        "output = llm(\"Â¿QuÃ© es la inteligencia artificial?\", max_tokens=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FDJHvxDmJFp",
        "outputId": "d581ef9b-b1b8-4cce-ab20-4f0cfc30ec3f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output['choices'][0]['text'].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhBzb6rkrWh1",
        "outputId": "95c89952-1524-4629-cc3b-8742d6670667"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Por quÃ© es importante para el futuro?\n",
            "\n",
            "La inteligencia artificial (IA) es una disciplina de la ciencia de la computaciÃ³n y la inteligencia artificial que se enfoca en el desarrollo de mÃ¡quinas y sistemas que pueden realizar tareas que requieren inteligencia humana, como reconocer patrones, tomar decisiones y aprender de experiencia. La IA se basa en la creaciÃ³n de algoritmos y modelos computacionales que permiten que estas mÃ¡quinas procesen informaciÃ³n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ” Prueba con otros modelos\n",
        "\n",
        "Ahora que ya sabes cÃ³mo cargar y usar un modelo `.gguf` con `llama-cpp-python`, puedes **repetir el proceso con cualquier otro modelo compatible**.\n",
        "\n",
        "### Â¿QuÃ© puedes probar?\n",
        "\n",
        "- Descargar otro modelo `.gguf` desde [Hugging Face](https://huggingface.co/models?library=ggml&sort=downloads).\n",
        "- Cargarlo con el mismo cÃ³digo que usamos para `Qwen3-1.7B`.\n",
        "- Cambiar el prompt, tokens o tamaÃ±o de contexto.\n",
        "- Medir diferencias en:\n",
        "  - â±ï¸ Tiempo de respuesta\n",
        "  - ðŸ“ Calidad de la salida\n",
        "  - ðŸ§  TamaÃ±o del modelo y uso de RAM\n"
      ],
      "metadata": {
        "id": "Uli3XeEDtu-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora vamos a descargar un modelo con 8b de parametros para probar la capacidad de google colab y el tiempo de respuesta\n",
        "!curl -L -o models/Meta-Llama-3-8B-Instruct.Q5_0.gguf  https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_0.gguf?download=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmD6jVjgt1K4",
        "outputId": "80aad13c-0c0e-4625-e682-fa89ad346a92"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1363  100  1363    0     0   9893      0 --:--:-- --:--:-- --:--:--  9948\n",
            "100 5339M  100 5339M    0     0  47.0M      0  0:01:53  0:01:53 --:--:-- 39.9M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=\"/content/models/Meta-Llama-3-8B-Instruct.Q5_0.gguf\",\n",
        "    n_ctx=512,\n",
        "    n_threads=4,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "output = llm(\"Â¿QuÃ© es la inteligencia artificial?\", max_tokens=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fe20UKCtuzc",
        "outputId": "fece198c-ff79-4168-d938-9dd10b693e4e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output['choices'][0]['text'].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fct4-GpMsyyV",
        "outputId": "0adb190b-7a05-4a4d-b24d-fd4f4a88bec8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Â¿QuÃ© es la inteligencia artificial? | Â¿QuÃ© es la inteligencia artificial? | Â¿QuÃ© es la inteligencia artificial? | Â¿QuÃ© es la inteligencia artificial?\n",
            "Â¿QuÃ© es la inteligencia artificial?\n",
            "La inteligencia artificial (IA) es el campo de la ciencia y la tecnologÃ­a que se enfoca en crear mÃ¡quinas que puedan realizar tareas que habitualmente requieren la inteligencia humana, como el aprendizaje, la toma de decisiones\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFmi9M23wQ4p",
        "outputId": "d8bb7952-4738-4343-dcf9-f48c3471d4a6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-91c097a6-15f7-4028-b9ac-73451a90bd00',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1752258741,\n",
              " 'model': '/content/models/Meta-Llama-3-8B-Instruct.Q5_0.gguf',\n",
              " 'choices': [{'text': ' | Â¿QuÃ© es la inteligencia artificial? | Â¿QuÃ© es la inteligencia artificial? | Â¿QuÃ© es la inteligencia artificial? | Â¿QuÃ© es la inteligencia artificial?\\nÂ¿QuÃ© es la inteligencia artificial?\\nLa inteligencia artificial (IA) es el campo de la ciencia y la tecnologÃ­a que se enfoca en crear mÃ¡quinas que puedan realizar tareas que habitualmente requieren la inteligencia humana, como el aprendizaje, la toma de decisiones',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'length'}],\n",
              " 'usage': {'prompt_tokens': 9, 'completion_tokens': 100, 'total_tokens': 109}}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## âœ… Conclusiones\n",
        "\n",
        "Acabas de correr un modelo de lenguaje local desde Google Colab, **sin GPU** y usando solo tu CPU y unos pocos GB de RAM.  \n",
        "Usaste `llama-cpp-python` para cargar un modelo `.gguf`, generar texto y entender cÃ³mo funcionan los LLMs a nivel bÃ¡sico.\n",
        "\n",
        "Esto puede parecer simple, pero no lo es.  \n",
        "Demuestra que **todos podemos experimentar con IA, incluso con recursos limitados**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ” Â¿Y ahora quÃ©?\n",
        "\n",
        "Si te quedÃ³ gustando, podÃ©s:\n",
        "\n",
        "- Probar otros modelos `.gguf` desde [TheBloke en HuggingFace](https://huggingface.co/TheBloke)\n",
        "- Ajustar los parÃ¡metros: `n_ctx`, `n_threads`, `max_tokens`\n",
        "- Hacer una app simple sobre esto: un bot, un script, un experimento\n",
        "- Compartir este notebook y mejorarlo con tu comunidad\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ™Œ Zenomy Labs\n",
        "\n",
        "Este taller es parte de un esfuerzo mÃ¡s grande:  \n",
        "**hacer crecer una comunidad de IA en LATAM que cree, experimente y comparta en abierto.**\n",
        "\n",
        "Si te gustÃ³ esto, sÃ­guenos en redes y sÃºmate a construir esta movida.  \n",
        "Nos faltan manos, ideas y energÃ­a.\n",
        "\n",
        "ðŸ‘‰ https://zenomyai.com\n",
        "\n",
        "---\n",
        "\n",
        "Gracias por llegar hasta acÃ¡ ðŸ™Œ\n"
      ],
      "metadata": {
        "id": "cwgpnnXcszSn"
      }
    }
  ]
}