# ðŸ§ª Taller #2 - Corre tu primer modelo local sin GPU con `llama.cpp`

Este es un taller prÃ¡ctico de **Zenomy Labs**, donde mostramos cÃ³mo correr modelos de lenguaje open source directamente desde Google Colab, usando solo CPU y menos de 12 GB de RAM.

---

## ðŸŽ¯ Â¿QuÃ© vas a aprender?

- QuÃ© es `llama.cpp` y por quÃ© es Ãºtil para correr modelos sin GPU.
- CÃ³mo descargar y usar modelos `.gguf` compatibles.
- CÃ³mo generar texto con `llama-cpp-python` desde un entorno sencillo.
- CÃ³mo comparar calidad, velocidad y uso de recursos entre modelos pequeÃ±os.

---

## ðŸš€ Â¿Por quÃ© hacemos esto?

En LatinoamÃ©rica muchas veces no tenemos acceso a GPUs o infraestructura de alto nivel.  
Pero eso no nos impide **explorar, aprender y construir IA localmente**.

Este taller busca **democratizar la experimentaciÃ³n con LLMs** y mostrar que sÃ­ es posible hacer IA desde el sur del mundo.

---

## ðŸ§° Requisitos

- Cuenta de Google (para usar Google Colab).
- Curiosidad tÃ©cnica.
- Nada de hardware especial.

---

## ðŸ““ Accede al taller

ðŸ‘‰ [Abrir notebook en Google Colab](https://colab.research.google.com/drive/1lPny_Ye9mhYq6ad0vD7V2ORuFO37YmJO?usp=sharing)

> Si el enlace estÃ¡ caÃ­do, puedes abrir directamente el archivo `notebook.ipynb` desde tu cuenta de Google Drive.

---

## ðŸ“¦ Modelos sugeridos

Todos en formato `.gguf` y compatibles con `llama.cpp`:

- [`Qwen3-1.7B`](https://huggingface.co/unsloth/Qwen3-1.7B-GGUF)
- [`TinyLlama 1.1B`](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF)
- [`Gemma 2B`](https://huggingface.co/TheBloke/gemma-2b-it-GGUF) (requiere mÃ¡s RAM)

---

## ðŸ§ª Ejemplo mÃ­nimo

```python
from llama_cpp import Llama

llm = Llama(model_path="models/Qwen3-1.7B-GGUF.gguf", n_ctx=512, n_threads=4)
output = llm("Â¿QuÃ© es la inteligencia artificial?", max_tokens=100)
print(output['choices'][0]['text'].strip())
